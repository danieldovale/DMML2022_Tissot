{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Custom Functions"
      ],
      "metadata": {
        "id": "l56aAvqkzK9Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook contains the documentation of custom functions we defined while building models. \n",
        "\n",
        "Here are the steps to import those functions:\n",
        "\n",
        "```python\n",
        "!pip install requests\n",
        "```\n",
        "\n",
        "```python\n",
        "import requests\n",
        "```\n",
        "\n",
        "```python\n",
        "url = 'https://raw.githubusercontent.com/danieldovale/DMML2022_Tissot/main/code/custom_functions.py'\n",
        "r = requests.get(url)\n",
        "\n",
        "\n",
        "with open('custom_functions.py', 'w') as f:\n",
        "    f.write(r.text)\n",
        "print(r.text)\n",
        "\n",
        "import custom_functions as cfun\n",
        "```\n",
        "In order to call those functions, use prefix cfun.\n",
        "> For example: \n",
        "```python \n",
        "cfun.evaluate(y_test, y_pred)\n",
        "```\n"
      ],
      "metadata": {
        "id": "rTA4_7UUyNUp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "J5iY-A2mzQQY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. evaluate()"
      ],
      "metadata": {
        "id": "a5FyTE4cpntz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```python\n",
        "def evaluate(true, pred):\n",
        "    precision = precision_score(true, pred, average = 'weighted')\n",
        "    recall = recall_score(true, pred, average = 'weighted')\n",
        "    f1 = f1_score(true, pred, average = 'weighted')\n",
        "    acc = accuracy_score(true, pred)\n",
        "    index = 'result'\n",
        "    d = {'accuracy': round(acc,4), 'precision': round(precision,4), 'recall': round(recall,4), 'f1 score': round(f1,4) }\n",
        "    df = pd.DataFrame(d,index=[\"results\"])\n",
        "    sns.heatmap(pd.DataFrame(confusion_matrix(true, pred)), annot=True, cmap='Oranges', fmt='.7g');\n",
        "    return df\n",
        "```\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "t0lfzkNvo3xq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The function `evaluate` takes in two arguments: `true` and `pred`. These are both lists or arrays of the same length, containing the true labels and predicted labels respectively for a classification task.\n",
        "\n",
        "The function then calculates the following evaluation metrics:\n",
        "\n",
        "* `precision`: the ratio of true positive predictions to all positive predictions    \n",
        "*`recall`: the ratio of true positive predictions to all actual positive instances              \n",
        "*`F1 score`: the harmonic mean of precision and recall                       \n",
        "*`accuracy`: the ratio of correct predictions to the total number of predictions           \n",
        "\n",
        "The function also generates a confusion matrix using the confusion_matrix function, which visualizes the number of true positive, true negative, false positive, and false negative predictions. Then the function plots a confusion matrix using the sns.heatmap function.\n",
        "\n",
        "The evaluation metrics are then stored in a dictionary and used to create a Pandas dataframe, which is returned by the function.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KsWaQa18p3q0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "530Yjjj2pscK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. prediction()"
      ],
      "metadata": {
        "id": "kFa2r-xSpl9k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```python\n",
        "def prediction(data, name, download = False):\n",
        "    df = pd.DataFrame(data = data)\n",
        "    df.index.names = ['id']\n",
        "    df.rename(columns = {0:'difficulty'}, inplace = True)\n",
        "    file_name = name + \".csv\"\n",
        "    df.to_csv(file_name)\n",
        "    if download == True:\n",
        "      files.download(file_name)\n",
        "    return df.head()\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "bPnp4aAopGBE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The prediction function takes in three arguments:\n",
        "\n",
        "* `data`: a list or array of data\n",
        "* `name`: a string representing the name to be used for the resulting CSV file\n",
        "* `download`: a boolean indicating whether or not to download the CSV file (defaults to False)\n",
        "The function first creates a Pandas dataframe from the input data, using the first element of each item in the data list as the index and the second element as the value in the 'difficulty' column. It then renames the index to 'id' and renames the '0' column to 'difficulty'.\n",
        "\n",
        "The function then saves the dataframe to a CSV file with the specified name. If the download argument is set to True, the function uses the files.download function from the Google Colab library to download the file. Finally, the function returns the first few rows of the dataframe."
      ],
      "metadata": {
        "id": "JT8wa_Xaq-6E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "lpkFzJRRp2qV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. spacy_tokenizer() and get_info"
      ],
      "metadata": {
        "id": "mwCRFvKtpw7k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```python\n",
        "def spacy_tokenizer(sentence):\n",
        "    doc = sp(sentence)\n",
        "    stop_words = nltk.corpus.stopwords.words(\"french\")\n",
        "    punctuations = string.punctuation\n",
        "  \n",
        "    mytokens = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in doc ]\n",
        "    mytokens = [ word for word in mytokens if word not in punctuations and word not in stop_words ]\n",
        "    return mytokens\n",
        "\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "rlEiCJGMpXpJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `spacy_tokenizer` function takes in a single argument: a string representing a sentence. It is used in the function `get_info` defined below.\n",
        "\n",
        "The function first uses the spacy library to process the sentence and create a doc object, which contains various information about the sentence, such as its parts of speech, dependencies, and lemmatized forms of the words.\n",
        "\n",
        "The function then defines two lists: `stop_words` and `punctuations`. The stop_words list contains French stop words, and the punctuations list contains all punctuation characters.\n",
        "\n",
        "The function then creates a list of tokens from the doc object and extracts the lemmatized and lowercase form of each word if it is not a pronoun, or the lowercase form of the word if it is a pronoun. The tokens are then filtered to remove any items that appear in the `punctuations` or `stop_words` lists."
      ],
      "metadata": {
        "id": "vn_2OYXcrdox"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```python\n",
        "def get_info(df):\n",
        " \n",
        "    text_length = []                          \n",
        "    number_of_sentences = []\n",
        "    number_of_words = []\n",
        "    sent_length_avg = []\n",
        "    words_length_avg = []\n",
        "    number_of_words_after_lemma_stop = []\n",
        "    longest_word_size = []\n",
        "    \n",
        "    for text in tqdm(df['sentence'].values):\n",
        "        \n",
        "      initial_length = len(text)\n",
        "      text_length.append(initial_length)\n",
        "\n",
        "      num_sentences = len(sent_tokenize(text))\n",
        "      number_of_sentences.append(num_sentences)\n",
        "        \n",
        "      punctuations = string.punctuation\n",
        "      text2 = text.lower()\n",
        "      text2 = word_tokenize(text2)\n",
        "      text2 = [word for word in text2 if word not in punctuations]\n",
        "      num_words = len(text2)\n",
        "      number_of_words.append(num_words)\n",
        "\n",
        "      sent_length_avg.append(num_words/num_sentences)\n",
        "        \n",
        "      words_length_avg.append(initial_length/num_words)\n",
        "\n",
        "      text = sp(text)\n",
        "      text = [word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in text]\n",
        "      text = [word for word in text if not word in spacy.lang.fr.stop_words.STOP_WORDS and word not in punctuations]\n",
        "\n",
        "      num_words_after_lemma_stop = len(text)\n",
        "      number_of_words_after_lemma_stop.append(num_words_after_lemma_stop)\n",
        "\n",
        "      word_len = [len(w) for w in text2]\n",
        "      longest_word_size.append(np.max(word_len))\n",
        "        \n",
        "    final_df = pd.concat([pd.Series(text_length), pd.Series(number_of_sentences),\n",
        "                             pd.Series(number_of_words), pd.Series(sent_length_avg),\n",
        "                             pd.Series(words_length_avg), pd.Series(number_of_words_after_lemma_stop),\n",
        "                             pd.Series(longest_word_size)], axis = 1)\n",
        "    final_df.columns = [\"text_length\", \"number_of_sentences\", \"number_of_words\",\n",
        "                           \"sent_length_avg\", \"words_length_avg\",\n",
        "                           \"number_of_words_after_lemma_stop\", \"longest_word_size\"]\n",
        "    \n",
        "    return final_df\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "XystTO5QpdOi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `get_info` function takes in a single argument: a Pandas dataframe `df` with a column called 'sentence'.\n",
        "\n",
        "The function initializes several empty lists to store various statistics about the sentences in the dataframe. It then iterates through the 'sentence' column of the dataframe using a progress bar provided by the `tqdm` function.\n",
        "\n",
        "For each sentence in the dataframe, the function performs the following operations:\n",
        "\n",
        "1. It calculates the initial length of the sentence and appends it to the \n",
        "`text_length` list.\n",
        "2. It uses the `sent_tokenize` function from the `nltk` library to split the sentence into individual sentences, and then counts the number of sentences. This value is appended to the `number_of_sentences` list.\n",
        "3. It tokenizes the sentence into words using the `word_tokenize` function from the `nltk` library, removes punctuation, and counts the number of words. This value is appended to the number_of_words list.\n",
        "4. It calculates the average number of words per sentence by dividing the `number_of_words` by the `number_of_sentences` and appends the result to the `sent_length_avg` list.\n",
        "5. It calculates the average length of the words in the sentence by dividing the `text_length` by the `number_of_words` and appends the result to the `words_length_avg` list.\n",
        "6. It processes the sentence using the `spacy` library to lemmatize the words and remove stop words. It then counts the number of remaining words and appends the result to the `number_of_words_after_lemma_stop` list.\n",
        "7. It calculates the length of each word in the sentence, finds the longest word, and appends its length to the `longest_word_size` list.\n",
        "\n",
        "After iterating through all the sentences in the dataframe, the function creates a new dataframe from the statistics lists and assigns appropriate column names. Finally, it returns the resulting dataframe."
      ],
      "metadata": {
        "id": "Mp4WsiTWsbMN"
      }
    }
  ]
}