{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Custom Functions"
      ],
      "metadata": {
        "id": "l56aAvqkzK9Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook contains the documentation of custom functions we defined while building models. \n",
        "\n",
        "Here are the steps to import those functions:\n",
        "\n",
        "```python\n",
        "!pip install requests\n",
        "```\n",
        "\n",
        "```python\n",
        "import requests\n",
        "```\n",
        "\n",
        "```python\n",
        "url = 'https://raw.githubusercontent.com/danieldovale/DMML2022_Tissot/main/code/custom_functions.py'\n",
        "r = requests.get(url)\n",
        "\n",
        "\n",
        "with open('custom_functions.py', 'w') as f:\n",
        "    f.write(r.text)\n",
        "print(r.text)\n",
        "\n",
        "import custom_functions as cfun\n",
        "```\n",
        "In order to call those functions, use prefix cfun.\n",
        "> For example: \n",
        "```python \n",
        "cfun.evaluate(y_test, y_pred)\n",
        "```\n"
      ],
      "metadata": {
        "id": "rTA4_7UUyNUp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "J5iY-A2mzQQY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. evaluate()"
      ],
      "metadata": {
        "id": "a5FyTE4cpntz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(true, pred):\n",
        "    precision = precision_score(true, pred, average = 'weighted')\n",
        "    recall = recall_score(true, pred, average = 'weighted')\n",
        "    f1 = f1_score(true, pred, average = 'weighted')\n",
        "    acc = accuracy_score(true, pred)\n",
        "    index = 'result'\n",
        "    d = {'accuracy': round(acc,4), 'precision': round(precision,4), 'recall': round(recall,4), 'f1 score': round(f1,4) }\n",
        "    df = pd.DataFrame(d,index=[\"results\"])\n",
        "    sns.heatmap(pd.DataFrame(confusion_matrix(true, pred)), annot=True, cmap='Oranges', fmt='.7g');\n",
        "    return df"
      ],
      "metadata": {
        "id": "iQa8-OgcBqex"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The function `evaluate` takes in two arguments: `true` and `pred`. These are both lists or arrays of the same length, containing the true labels and predicted labels respectively for a classification task.\n",
        "\n",
        "The function then calculates the following evaluation metrics:\n",
        "\n",
        "* `precision`: the ratio of true positive predictions to all positive predictions    \n",
        "* `recall`: the ratio of true positive predictions to all actual positive instances              \n",
        "* `F1 score`: the harmonic mean of precision and recall                       \n",
        "* `accuracy`: the ratio of correct predictions to the total number of predictions           \n",
        "\n",
        "The function also generates a confusion matrix using the confusion_matrix function, which visualizes the number of true positive, true negative, false positive, and false negative predictions. Then the function plots a confusion matrix using the sns.heatmap function.\n",
        "\n",
        "The evaluation metrics are then stored in a dictionary and used to create a Pandas dataframe, which is returned by the function.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KsWaQa18p3q0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "530Yjjj2pscK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. prediction()"
      ],
      "metadata": {
        "id": "kFa2r-xSpl9k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prediction(data, name, download = False):\n",
        "    df = pd.DataFrame(data = data)\n",
        "    df.index.names = ['id']\n",
        "    df.rename(columns = {0:'difficulty'}, inplace = True)\n",
        "    file_name = name + \".csv\"\n",
        "    df.to_csv(file_name)\n",
        "    if download == True:\n",
        "      files.download(file_name)\n",
        "    return df.head()"
      ],
      "metadata": {
        "id": "hxWvp8ZEBupb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The prediction function takes in three arguments:\n",
        "\n",
        "* `data`: a list or array of data\n",
        "* `name`: a string representing the name to be used for the resulting CSV file\n",
        "* `download`: a boolean indicating whether or not to download the CSV file (defaults to False)\n",
        "The function first creates a Pandas dataframe from the input data, using the first element of each item in the data list as the index and the second element as the value in the 'difficulty' column. It then renames the index to 'id' and renames the '0' column to 'difficulty'.\n",
        "\n",
        "The function then saves the dataframe to a CSV file with the specified name. If the download argument is set to True, the function uses the files.download function from the Google Colab library to download the file. Finally, the function returns the first few rows of the dataframe."
      ],
      "metadata": {
        "id": "JT8wa_Xaq-6E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "lpkFzJRRp2qV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. class_accuracy() "
      ],
      "metadata": {
        "id": "qgDEZ8RC-nA-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def class_accuracy(y_test, y_pred):\n",
        "  unique, counts = np.unique(y_test, return_counts=True)\n",
        "  target = dict(zip(unique, counts))\n",
        "  diagonal = confusion_matrix(y_test, y_pred).diagonal()\n",
        "  percentages = diagonal / np.array(list(target.values()))\n",
        "  for index, key in enumerate(target.keys()):\n",
        "    print(f'{key}: {percentages[index]:.2f}%')"
      ],
      "metadata": {
        "id": "rIytDH9GBw8x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function calculates accuracy for each classes. It takes in two input arguments:\n",
        "\n",
        "1. `y_test`: array of true labels for the test data\n",
        "2. `y_pred`: array of predicted labels for the test data\n",
        "\n",
        "The function first uses np.unique to find the unique labels in the y_test array, as well as their counts. It stores these values in the variables unique and counts, respectively.\n",
        "\n",
        "Next, the function creates a dictionary, target, that maps each unique label to its count. It does this using the zip function to combine the two arrays unique and counts into a single iterable.\n",
        "\n",
        "Then, the function calculates the diagonal of the confusion matrix for the test data using the confusion_matrix function from scikit-learn. The diagonal of a confusion matrix represents the number of instances that were correctly classified for each class.\n",
        "\n",
        "The function then divides the values in the diagonal by the corresponding class counts in the target dictionary to compute the class accuracy percentages.\n",
        "\n",
        "Finally, the function iterates over the keys in the target dictionary and prints out the class accuracy percentage for each class."
      ],
      "metadata": {
        "id": "St2FdR75-x1Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "voRFL_RU_OYT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. pred_compare_df() and compare()"
      ],
      "metadata": {
        "id": "48S37l1b_PMV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pred_compare_df(X_test, y_test, y_pred):\n",
        "    tempdf_1 = pd.concat([X_test, y_test], axis = 1).reset_index(drop=True)\n",
        "    y_pred_df = pd.Series(y_pred)\n",
        "    tempdf_2 = pd.concat([tempdf_1, y_pred_df], axis =1).rename(columns = {0: 'predicted difficulty'}) \n",
        "    tempdf_3 = pd.Series(tempdf_2['difficulty'] == tempdf_2['predicted difficulty'])\n",
        "    final_df = pd.concat([tempdf_2, tempdf_3], axis = 1).rename(columns = {0: 'correct prediction'}) \n",
        "    return final_df"
      ],
      "metadata": {
        "id": "I2MVxDw3BzOx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function compares the true labels and predicted labels for a classification task and returns a data frame that contains the test data and the true and predicted labels. It takes in three input arguments:\n",
        "\n",
        "1. `X_test`: a data frame containing the feature values for the test data\n",
        "2. `y_test`: one dimensional dataframe containing the true labels for the test data\n",
        "3. `y_pred`: one dimensional dataframe containing the predicted labels for the test data\n",
        "\n",
        "The function starts by concatenating the `X_test` and `y_test` data frames along the columns axis. It then resets the index of the resulting data frame using the `reset_index` function and drops the old index. This results in a new data frame, `tempdf_1`, that contains the test data and the true labels.\n",
        "\n",
        "Next, it concatenates `y_pred` with tempdf_1 along the columns axis to create a new data frame, `tempdf_2`, that contains the test data, the true labels, and the predicted labels. The function then renames the third column in tempdf_2 to 'predicted difficulty'.\n",
        "\n",
        "The function then creates a new series, `tempdf_3`, that contains a boolean value for each row in `tempdf_2`, indicating whether the true label and predicted label for that row are equal. It does by comparing the 'difficulty' and 'predicted difficulty' columns in `tempdf_2`.\n",
        "\n",
        "Finally, the function concatenates `tempdf_2` and `tempdf_3` to create the final data frame, `final_df`, and renames the third column to 'correct prediction'. The function then returns final_df."
      ],
      "metadata": {
        "id": "Zan67sWv_U3V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compare(df, i):\n",
        "  temp = df.iloc[i]\n",
        "  print(\"sentence:\\t\\t%s\\ndifficulty:\\t\\t%s\\npredicted difficulty:\\t%s\\ncorrect prediction:\\t%s\" % (temp[\"sentence\"], \n",
        "  temp[\"difficulty\"], \n",
        "  temp[\"predicted difficulty\"], \n",
        "  temp['correct prediction']))"
      ],
      "metadata": {
        "id": "Vujk9CpJB1MS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function takes in a data frame and an integer index, and prints out a comparison of the true label and predicted label for the row at the specified index in the data frame. It is intended to be used in conjunction with the `pred_compare_df` function.\n",
        "\n",
        "The function first uses the iloc attribute to select the row at the specified index in the data frame. It then stores this row in a new variable, temp.\n",
        "\n",
        "Next, the function uses a formatted string to print out the sentence, true label, predicted label, and a boolean value indicating whether the prediction was correct. It does this by using the `%` operator to insert the values of the corresponding columns in the temp data frame into the string.\n",
        "\n",
        "The `\\t` characters in the string are escape sequences that add tab characters to the output, which helps to align the columns of the output."
      ],
      "metadata": {
        "id": "4Z3QlUWRBDf4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "P7mQ83U3BRT3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. get_info()"
      ],
      "metadata": {
        "id": "mwCRFvKtpw7k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_info(df):\n",
        "  \n",
        "    text_length = []                          \n",
        "    number_of_sentences = []\n",
        "    number_of_words = []\n",
        "    sent_length_avg = []\n",
        "    words_length_avg = []\n",
        "    number_of_words_after_lemma_stop = []\n",
        "    longest_word_size = []\n",
        "    \n",
        "    for text in tqdm(df['sentence'].values):\n",
        "        \n",
        "      initial_length = len(text)\n",
        "      text_length.append(initial_length)\n",
        "\n",
        "      num_sentences = len(sent_tokenize(text))\n",
        "      number_of_sentences.append(num_sentences)\n",
        "        \n",
        "      punctuations = string.punctuation\n",
        "      text2 = text.lower()\n",
        "      text2 = word_tokenize(text2)\n",
        "      text2 = [word for word in text2 if word not in punctuations]\n",
        "      num_words = len(text2)\n",
        "      number_of_words.append(num_words)\n",
        "\n",
        "      sent_length_avg.append(num_words/num_sentences)\n",
        "        \n",
        "      words_length_avg.append(initial_length/num_words)\n",
        "\n",
        "      text = sp(text)\n",
        "      text = [word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in text]\n",
        "      text = [word for word in text if not word in spacy.lang.fr.stop_words.STOP_WORDS and word not in punctuations]\n",
        "\n",
        "      num_words_after_lemma_stop = len(text)\n",
        "      number_of_words_after_lemma_stop.append(num_words_after_lemma_stop)\n",
        "\n",
        "      word_len = [len(w) for w in text2]\n",
        "      longest_word_size.append(np.max(word_len))\n",
        "        \n",
        "    final_df = pd.concat([pd.Series(text_length), pd.Series(number_of_sentences),\n",
        "                             pd.Series(number_of_words), pd.Series(sent_length_avg),\n",
        "                             pd.Series(words_length_avg), pd.Series(number_of_words_after_lemma_stop),\n",
        "                             pd.Series(longest_word_size)], axis = 1)\n",
        "    final_df.columns = [\"text_length\", \"number_of_sentences\", \"number_of_words\",\n",
        "                           \"sent_length_avg\", \"words_length_avg\",\n",
        "                           \"number_of_words_after_lemma_stop\", \"longest_word_size\"]\n",
        "    \n",
        "    return final_df"
      ],
      "metadata": {
        "id": "yw2zljVnB3F8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `get_info` function takes in a single argument: a Pandas dataframe `df` with a column called 'sentence'.\n",
        "\n",
        "The function initializes several empty lists to store various statistics about the sentences in the dataframe. It then iterates through the 'sentence' column of the dataframe using a progress bar provided by the `tqdm` function.\n",
        "\n",
        "For each sentence in the dataframe, the function performs the following operations:\n",
        "\n",
        "1. It calculates the length (in characters) of the text and appends it to the `text_length` list.\n",
        "\n",
        "2. It uses the `sent_tokenize` function from the `nltk` library to split the text into individual sentences, and then counts the number of sentences. This value is appended to the `number_of_sentences` list.\n",
        "\n",
        "3. It tokenizes the text into words using the `word_tokenize` function from the `nltk` library, removes punctuation, and counts the number of words. This value is appended to the number_of_words list.\n",
        "\n",
        "4. It calculates the average number of words per sentence by dividing the `number_of_words` by the `number_of_sentences` and appends the result to the `sent_length_avg` list.\n",
        "\n",
        "5. It calculates the average length of the words in the text by dividing the `text_length` by the `number_of_words` and appends the result to the `words_length_avg` list.\n",
        "\n",
        "6. It processes the text using the `spacy` library to lemmatize the words and remove stop words. It then counts the number of remaining words and appends the result to the `number_of_words_after_lemma_stop` list.\n",
        "\n",
        "7. It calculates the length of each word in the sentence, finds the longest word, and appends its length to the `longest_word_size` list.\n",
        "\n",
        "After iterating through all the sentences in the dataframe, the function creates a new dataframe from the statistics lists and assigns appropriate column names. Finally, it returns the resulting dataframe."
      ],
      "metadata": {
        "id": "Mp4WsiTWsbMN"
      }
    }
  ]
}